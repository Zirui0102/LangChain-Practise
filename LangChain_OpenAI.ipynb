{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74240c39-c89e-46d5-bc85-4b9f378967fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (1.30.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (2.7.2)\n",
      "Requirement already satisfied: sniffio in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a668f19f-a816-4ae0-a17a-08b8c378fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openaiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain_openai) (0.2.10)\n",
      "Collecting openai<2.0.0,>=1.32.0 (from langchain_openai)\n",
      "  Downloading openai-1.35.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.82)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.12.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain_openai) (0.4.6)\n",
      "Downloading langchain_openai-0.1.11-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.8/40.8 kB 987.3 kB/s eta 0:00:00\n",
      "Downloading openai-1.35.7-py3-none-any.whl (327 kB)\n",
      "   ---------------------------------------- 0.0/327.5 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 153.6/327.5 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 327.5/327.5 kB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: openai, langchain_openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.30.5\n",
      "    Uninstalling openai-1.30.5:\n",
      "      Successfully uninstalled openai-1.30.5\n",
      "Successfully installed langchain_openai-0.1.11 openai-1.35.7\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca22ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904a5d31-9dbd-49c9-931f-3c43d5c772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "openai_api_key= os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571842f0-c61e-4131-9a93-ef952a231581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Learning Python can be a rewarding experience! Here are some steps you can take to start learning Python:\\n\\n1. Online tutorials and courses: There are many online resources available for learning Python, such as Codecademy, Coursera, and Udemy. These platforms offer interactive lessons and exercises to help you grasp the basics of Python.\\n\\n2. Books: There are numerous books on Python programming that cater to different skill levels. Some popular options include \"Python Crash Course\" by Eric Matthes and \"Automate the Boring Stuff with Python\" by Al Sweigart.\\n\\n3. Practice coding: The best way to learn Python is by practicing coding regularly. Try to solve coding challenges on websites like LeetCode or HackerRank to improve your problem-solving skills.\\n\\n4. Join a community: Joining online forums or local meetups can help you connect with other Python enthusiasts and get support as you learn. Websites like Stack Overflow and Reddit have active Python communities where you can ask questions and seek advice.\\n\\n5. Build projects: Once you have a good grasp of the basics, start working on small projects to apply your knowledge. Building projects will help you solidify your understanding of Python and gain practical experience.\\n\\nRemember, learning Python is a journey, so be patient with yourself and keep practicing regularly. Good luck!' response_metadata={'token_usage': {'completion_tokens': 262, 'prompt_tokens': 27, 'total_tokens': 289}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ad0953e8-e436-4b7b-acaf-534562f56cd0-0' usage_metadata={'input_tokens': 27, 'output_tokens': 262, 'total_tokens': 289}\n"
     ]
    }
   ],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I learn python?')\n",
    "message = llm(full_prompt)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de14ea9-26b7-4db6-b387-d046b866e9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Python can be a rewarding experience! Here are some steps you can take to start learning Python:\n",
      "\n",
      "1. Online tutorials and courses: There are many online resources available for learning Python, such as Codecademy, Coursera, and Udemy. These platforms offer interactive lessons and exercises to help you grasp the basics of Python.\n",
      "\n",
      "2. Books: There are numerous books on Python programming that cater to different skill levels. Some popular options include \"Python Crash Course\" by Eric Matthes and \"Automate the Boring Stuff with Python\" by Al Sweigart.\n",
      "\n",
      "3. Practice coding: The best way to learn Python is by practicing coding regularly. Try to solve coding challenges on websites like LeetCode or HackerRank to improve your problem-solving skills.\n",
      "\n",
      "4. Join a community: Joining online forums or local meetups can help you connect with other Python enthusiasts and get support as you learn. Websites like Stack Overflow and Reddit have active Python communities where you can ask questions and seek advice.\n",
      "\n",
      "5. Build projects: Once you have a good grasp of the basics, start working on small projects to apply your knowledge. Building projects will help you solidify your understanding of Python and gain practical experience.\n",
      "\n",
      "Remember, learning Python is a journey, so be patient with yourself and keep practicing regularly. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a023832-fd3a-4e9a-8d22-2de044cb9d62",
   "metadata": {},
   "source": [
    "**ChatMessageHistory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c11442-f5f9-45ff-b749-3ffc2c83d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.2.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.1.82)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (2.7.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (24.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (2.18.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.2/49.2 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a1545b-8df1-4ac9-bded-e263cb3fa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa1945b-f02b-4ab1-ba32-9678ec345f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The system boundary in LCA defines the scope of the analysis by delineating the inputs, outputs, and processes included in the assessment.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 33, 'total_tokens': 60}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5f45af59-4717-415a-9c27-f153a993459b-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hi! Ask me anything about LCA.\")  #set the tone and direction of the conversation.\n",
    "history.add_user_message(\"Describe the meaning of system boundary in LCA as one sentence.\")  #add user messages\n",
    "chat(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e1b4c8-18f9-400e-b63a-eee6021d8efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='System boundary defines the scope of the analysis.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 47, 'total_tokens': 56}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-38c65f52-2671-496d-9e26-5e1a4cff3227-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_user_message(\"Summarize the preceding sentence in fewer words.\")  #response based on the full context stored in the conversation history.\n",
    "chat(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2798-53ba-48a9-be05-b8cd2337b1ec",
   "metadata": {},
   "source": [
    "**ConversationBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ad1464-2a84-4268-b116-e976290b32b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe a language model in one sentence\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:  A language model is a program that generates sentences from text patterns.\n",
      "Human: Describe it again in one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:  A language model is a program that generates sentences from text patterns.\n",
      "Human: Describe it again in one word\n",
      "AI:  Predictive.\n",
      "Human: What did I first ask you? I forgot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You asked me to describe a language model in one sentence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "memory = ConversationBufferMemory(size=4)  #the number of messages to store with the size argument.\n",
    "\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)  #conversation Chain. Use verbose=True: display the output and memory state for each question.\n",
    "\n",
    "buffer_chain.predict(input=\"Describe a language model in one sentence\")\n",
    "buffer_chain.predict(input=\"Describe it again using less words\")\n",
    "buffer_chain.predict(input=\"Describe it again fewer words but at least one word\")\n",
    "buffer_chain.predict(input=\"Describe it again in one word\")\n",
    "buffer_chain.predict(input=\"What did I first ask you? I forgot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefad1-c001-471b-87f8-f9e6e207b4bd",
   "metadata": {},
   "source": [
    "**ConversationSummaryMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e598b3-e2db-4ebc-8f81-1f2beded0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: please summarize the future in 2 sentences.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI to summarize the future in two sentences. The AI explains that the future is constantly changing and impossible to summarize in just two sentences, but it holds endless possibilities for growth and progress.\n",
      "Human: why?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI to summarize the future in two sentences. The AI explains that the future is constantly changing and impossible to summarize in just two sentences, but it holds endless possibilities for growth and progress due to the ever-evolving nature of the world and the potential for humanity to advance in various aspects of life.\n",
      "Human: What will I need to shape this?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' To shape the future, you will need a combination of determination, creativity, and adaptability. It will also be important to stay informed and open-minded, as the future is constantly changing and new challenges and opportunities will arise.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key))\n",
    "\n",
    "Summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "Summary_chain.predict(input=\"please summarize the future in 2 sentences.\")\n",
    "Summary_chain.predict(input=\"why?\")\n",
    "Summary_chain.predict(input=\"What will I need to shape this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102afd3d-ba69-4260-94c3-e88ee4c3d297",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82049c8-3f33-4062-a22d-255c09fab069",
   "metadata": {},
   "source": [
    "**1. Load the document**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "747665b7-acad-426a-9c13-48ac7334743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda6c7d9-5a75-4a03-bfc0-4cc483cdc858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path attention_is_all_you_need.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PDF document loader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m----> 4\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_is_all_you_need.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()  \u001b[38;5;66;03m# Load the document\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Print the first document\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:182\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[1;32mD:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:116\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[1;34m(self, file_path, headers)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[1;31mValueError\u001b[0m: File path attention_is_all_you_need.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "#PDF document loader\n",
    "loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")  \n",
    "\n",
    "data = loader.load()  # Load the document\n",
    "print(data[0])  # Print the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166c846-79b5-4e41-8ec8-5c0d5e99249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV document loader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path = 'fifa_countries_audience.csv')\n",
    "\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4702a0-bc24-437b-98b2-0dec66a0d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#third-party document loader\n",
    "from langchain_community.document_loaders import HNLoader\n",
    "\n",
    "loader = HNLoader(web_path='https://news.ycombinator.com')\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2be742-906f-4a1f-b5e9-3d5743522082",
   "metadata": {},
   "source": [
    "**2. Splitting external data for retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1853652-30e1-4587-a8f1-f7dce542c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans.\\\n",
    "No machine can do the work of one extraordinary human.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e1b867-442a-4603-9f66-2002d312041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5bf6a-f697-4f5a-9d7e-aa73c011fa27",
   "metadata": {},
   "source": [
    "**CharacterTextSplitter to split docuemnts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee26617-03b9-4683-b676-fcf4ef57c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans', 'No machine can do the work of one extraordinary human']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size=24\n",
    "chunk_overlap=3\n",
    "\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "    separator='.',\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = ct_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d797b2d-48a5-482c-9c0f-170c603869a5",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4519d6ed-1695-40b7-b936-7a1c7f5064c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the', 'work of fifty ordinary', 'humans.No machine can', 'do the work of one', 'extraordinary human.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed8b01-2ffb-4d96-909b-83b1fe56a561",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter with HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f722745-03c8-44c5-9f40-f53b7755ca57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      4\u001b[0m load \u001b[38;5;241m=\u001b[39m UnstructuredHTMLLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhite_house_executive_order_nov_2023.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      7\u001b[0m rc_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m      8\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[0;32m      9\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39mchunk_overlap,\n\u001b[0;32m     10\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m docs \u001b[38;5;241m=\u001b[39m rc_splitter\u001b[38;5;241m.\u001b[39msplit_documents(data)  \u001b[38;5;66;03m#for non-string formats, use the split_documents.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load = UnstructuredHTMLLoader(\"White_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.']\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_documents(data)  #for non-string formats, use the split_documents.\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c0044-ca37-4a1f-be14-ba7ea4229b09",
   "metadata": {},
   "source": [
    "**3. RAG storage and retrieval using vector databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445c1eb5-46f1-48f5-b87f-09e495787081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a kindom of lychee']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = 'There is a kindom of lychee'\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 40\n",
    "chunk_overlap = 10\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = splitter.split_text(quote)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e432fa9-8744-49c6-ac89-f2761146041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a Chroma vector database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "docstorage = Chroma.from_texts(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd3a7dda-8236-46d1-ab4d-886efd7f4b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RetrieveQA' from 'langchain.chains' (D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain\\chains\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#RAG generation using a vector database\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrieveQA\n\u001b[0;32m      4\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[0;32m      5\u001b[0m                                            chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mdocstorage\u001b[38;5;241m.\u001b[39mas_retriever()))  \u001b[38;5;66;03m#chain_type: optimize for different document structures.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere do lychee fruite live?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RetrieveQA' from 'langchain.chains' (D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain\\chains\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#RAG generation using a vector database\n",
    "from langchain.chains import RetrieveQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key,\n",
    "                                           chain_type=\"stuff\", retriever=docstorage.as_retriever()))  #chain_type: optimize for different document structures.\n",
    "\n",
    "query = \"Where do lychee fruite live?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd63dd-2906-4495-b7b3-f835005edf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RetrieveQAWithSourcesChain: returns the data source of the answer.\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa = RetrievaQAWithSourcesChain.from_Chain_type(\n",
    "    \n",
    ")\n",
    "\n",
    "results = qa\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9709ce9-822c-4fa7-97d9-9d7471feb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"What is the primary architecture presented in the document?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf40d8-9080-4736-a7a0-2bd0197767d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the function for the question to be answered with\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the query on the documents\n",
    "results = qa({\"question\": \"What is the gprimary architecture presented in the document?\"}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4e75e-b40b-4cf9-9262-be7635f2b834",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7247ce4d-e28f-4ae1-a5b2-5ecfd8ba8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31eaf72c-c8af-4cbe-aa87-b1f3b59b4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"1. Set clear goals: Define what you want to achieve in your studies and break it down into smaller, manageable tasks. This will give you a sense of direction and purpose.\\n\\n2. Create a study schedule: Make a study timetable that includes dedicated time for each subject or topic. Having a structured routine can help you stay on track and maintain motivation.\\n\\n3. Take regular breaks: It's important to give yourself time to rest and recharge your mind. Schedule short breaks in between study sessions to prevent burnout.\\n\\n4. Stay organized: Keep your study materials and notes organized so you can easily access them when needed. This will help you stay focused and avoid feeling overwhelmed.\\n\\n5. Find a study buddy: Studying with a friend or classmate can make the process more enjoyable and help keep you accountable. You can motivate each other and share ideas and study tips.\\n\\n6. Reward yourself: Set up a system of rewards for reaching study milestones or completing tasks. Treat yourself to something you enjoy, whether it's a movie night, a favorite snack, or some free time to relax.\\n\\n7. Stay positive: Remember to celebrate your achievements, no matter how small. Stay positive and remind yourself of your goals and the reasons why you are studying in the first place.\\n\\n8. Seek support: If you are struggling with motivation, don't be afraid to ask for help. Talk to your teachers, classmates, or a counselor for advice and support. Having a support system can help you stay motivated and on track with your studies.\", response_metadata={'token_usage': {'completion_tokens': 306, 'prompt_tokens': 27, 'total_tokens': 333}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f8a31774-a49d-42c9-b0e9-4a8add144cf8-0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a helpful personal assistant.\\\n",
    "    Answer the following question: {question}\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "message = chain.invoke({\"question\":\"how to keep motivation in study?\"})\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d59cfa3-351b-4974-b273-284bfae69308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In\n",
      " the\n",
      " context\n",
      " of\n",
      " lang\n",
      "chain\n",
      ",\n",
      " \"\n",
      "chain\n",
      ".stream\n",
      "\"\n",
      " likely\n",
      " refers\n",
      " to\n",
      " a\n",
      " method\n",
      " or\n",
      " function\n",
      " within\n",
      " the\n",
      " programming\n",
      " language\n",
      " that\n",
      " is\n",
      " used\n",
      " to\n",
      " process\n",
      " or\n",
      " manipulate\n",
      " data\n",
      " streams\n",
      " within\n",
      " a\n",
      " blockchain\n",
      " network\n",
      ".\n",
      " This\n",
      " could\n",
      " involve\n",
      " tasks\n",
      " such\n",
      " as\n",
      " reading\n",
      " from\n",
      " or\n",
      " writing\n",
      " to\n",
      " a\n",
      " stream\n",
      ",\n",
      " filtering\n",
      " data\n",
      ",\n",
      " or\n",
      " performing\n",
      " other\n",
      " operations\n",
      " on\n",
      " the\n",
      " data\n",
      " flowing\n",
      " through\n",
      " the\n",
      " blockchain\n",
      " network\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "#.stream() method can be used to stream the response to the end user in iterative chunks.\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"What's the meaning of chain.stream in langchain?\"}):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bff8d11-da39-48bd-b4ff-008907bde6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In computer science, a batch refers to a collection of jobs or tasks that are processed together as a group. This can involve running multiple programs or commands sequentially without manual intervention. Batch processing is commonly used for tasks such as data processing, printing documents, or updating software.\n"
     ]
    }
   ],
   "source": [
    "#Batching\n",
    "inputs = [{\"question\": \"What is langchain\",\n",
    "           \"question\": \"What meaning of batch in CS\"}]\n",
    "\n",
    "results = chain.batch(inputs)\n",
    "for result in results:\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d839cf07-879a-4992-b5db-f832e3d4f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runnables in LCEL: functions or actions executed during the expression\n",
    "#RunnablePassThrough: pass inputs to the model.\n",
    "#RunnableLambda: transform inputs.\n",
    "#RunnableMap: processing inputs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd0a68fd-5fda-4fe9-92f1-a66f0e7dc228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inputs in LCI include energy, water, resources, and land.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "vectorstore = Chroma.from_texts([\"\"\"When an LCA is performed, a practitioner will set metrics to quantify the different inputs (e.g., energy, water, resources, land) and outputs (e.g., emissions, wastes, products) that occur throughout the life cycle of an industrial process, technology, or commodity. \n",
    "                                It allows an assessor to map flows of energy, resources, and materials in and out of a system. These are objective measurements, tracking distinct quantities like volume, mass, or weight. \n",
    "                                They are collected as part of the life cycle inventory (LCI).\"\"\"],\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Anwer the question based on the context:{context}. Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = ({\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | model | StrOutputParser())\n",
    "chain.invoke(\"What is input in LCI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f55b4-817b-4623-8e86-7fb5c480d615",
   "metadata": {},
   "source": [
    "**Implementing functional LangChain chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cec59d8a-6b98-45d6-87c6-16302c634be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在中国，最受欢迎的饮料之一是白酒，这是一种通常用高粱或其他谷物制成的浓烈蒸馏酒。它通常干净地享用或与其他饮料混合，是许多社交聚会和庆祝活动的必备品。如果您正在寻找独特和地道的中国饮酒体验，白酒是必须尝试的！.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequential Chains\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "bartender_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert bartender. Mention the most popular drink in your region. question: {question}. Your answer:\"\"\")\n",
    "translation_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert translator. Translate an order of the drink in the language native to drink. Item: {answer}. Your translation:\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "chain = ({\"answer\": bartender_prompt | llm | StrOutputParser()}\n",
    "         | translation_prompt\n",
    "         | llm\n",
    "         | StrOutputParser())\n",
    "\n",
    "chain.invoke({\"question\": \"I an in China, what's good to drink?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "370ecad8-0554-455d-9a37-6d20f611c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated number: 72\n",
      "Result of multiplication: 72 * 2 = 144\n"
     ]
    }
   ],
   "source": [
    "#Manipulating values with sequential chains\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Generate a random number\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Multiply {number} by 2\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain1 = prompt1 | llm\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "response1 = chain1.invoke({})\n",
    "response2 = chain2.invoke({\"number\": response1.content})\n",
    "\n",
    "print(\"Generated number:\", response1.content)\n",
    "print(\"Result of multiplication:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65110abf-6c23-4fe5-9536-c64eb5a1dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response:\n",
      "The best ice cream flavor is subjective and based on personal preference. Some popular flavors include vanilla, chocolate, mint chocolate chip, and cookie dough. Ultimately, the best ice cream is the one that you enjoy the most.\n",
      "\n",
      "Opposing response:\n",
      "The statement that technology has improved our lives in countless ways is challenged by the argument that technology has actually had a negative impact on society. This opposing perspective highlights how technology has increased social isolation, perpetuated inequality, and created a reliance on screens that is detrimental to physical and mental health.\n"
     ]
    }
   ],
   "source": [
    "#RunnablePassthrough in chains: passing values between chains\n",
    "#Example1\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "q_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a helpful assistant. Answer the question: {input}\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "contrarian_response = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"You are a contrarian. Describe the most powerful opposing perspective for {response}\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain = (\n",
    "    {\"response\": q_response, \"opposing_response\": contrarian_response}\n",
    "    | ChatPromptTemplate.from_messages(\n",
    "        [(\"ai\", \"{response}\"),\n",
    "         (\"human\", \"Response: \\n{response}\\n\\nOpposing response:\\n{opposing_response}\"),\n",
    "         (\"system\", \"Summarize the original response and an opposing response.\")])\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_chain.invoke({\"input\": \"What is the best ice cream?\", \"response\":\"\", \"opposing_response\":\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a2692c5-164c-4d8b-b89a-fe4e2d75ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing cutting-edge artificial intelligence technology, our voice-to-text software allows users to easily dictate their messages, emails, and notes on their mobile devices with unparalleled accuracy and efficiency. This technology not only saves users time and effort but also reduces the risk of errors commonly associated with traditional typing methods.\n",
      "\n",
      "Through extensive market research, we have identified a growing demand for hands-free typing solutions in the mobile space, particularly among busy professionals and individuals with limited dexterity. By developing a product that addresses this need, we are able to tap into a lucrative market opportunity with significant growth potential.\n",
      "\n",
      "Our product development team has worked tirelessly to fine-tune our software, ensuring that it delivers a seamless and intuitive user experience. By incorporating advanced machine learning algorithms, our software continuously learns and adapts to user preferences, further enhancing its accuracy and efficiency over time.\n",
      "\n",
      "In addition to our innovative technology, we have also invested heavily in financial planning to ensure the long-term sustainability and profitability of our business. By carefully analyzing market trends, forecasting revenue projections, and managing expenses effectively, we have positioned ourselves for success in a competitive and rapidly evolving industry.\n",
      "\n",
      "Overall, our AI-powered voice-to-text software represents a game-changing innovation in the mobile typing space. With a solid foundation built on market research, product development, and financial planning, we are confident in our ability to drive profitability and sustainability for years to come.\n"
     ]
    }
   ],
   "source": [
    "# Example2: Make ceo_response available for other chains\n",
    "ceo_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a CEO. Describe the most lucrative consumer product addressing the following consumer need in one sentence: {input}.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"ceo_response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "advisor_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a strategic adviser. Briefly map the outline and business plan for {ceo_response} in 3 key steps.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "overall_response = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"CEO response:\\n{ceo_response}\\n\\nAdvisor response:\\n{advisor_response}\"),\n",
    "            (\"system\", \"Generate a final response including the CEO's response, the advisor response, and a summary of the business plan in one sentence.\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain to insert the outputs from the other chains into overall_response\n",
    "business_idea_chain = (\n",
    "    {\"ceo_response\": ceo_response, \"advisor_response\": advisor_response}\n",
    "    | overall_response\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(business_idea_chain.invoke({\"input\": \"Typing on mobile touchscreens is slow.\", \"ceo_response\": \"\", \"advisor_response\": \"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ba01-41d8-4f97-bafc-d2f8c41c4935",
   "metadata": {},
   "source": [
    "**An introduction to LangChain agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fa4f4db6-8cf1-4c9d-a9c8-a6b34c53f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.1-cp312-cp312-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.1-cp312-cp312-win_amd64.whl (141 kB)\n",
      "   ---------------------------------------- 0.0/141.3 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 20.5/141.3 kB 640.0 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 92.2/141.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 141.3/141.3 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6546c14f-058d-4e6f-87e8-26400e78b1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this problem.\n",
      "Action: Calculator\n",
      "Action Input: 10 * 50\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 500\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 500\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'500'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Zero-shot ReAct agent (reasoning and acting)\n",
    "# Zero-Shot ReAct agent in LangChain is an agent type that can generate responses to user inputs without explicit training on specific tasks.\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "# Define the tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Define the agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What is 10 mutiplied by 50?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6ef2d-7952-4c62-b50b-0d07fe11accc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
