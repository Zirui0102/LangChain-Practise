{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca22ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904a5d31-9dbd-49c9-931f-3c43d5c772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "openai_api_key= os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571842f0-c61e-4131-9a93-ef952a231581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Learning Python can be a rewarding experience! Here are some steps you can take to start learning Python:\\n\\n1. Online tutorials and courses: There are many online resources available for learning Python, such as Codecademy, Coursera, and Udemy. These platforms offer interactive lessons and exercises to help you grasp the basics of Python.\\n\\n2. Books: There are numerous books on Python programming that cater to different skill levels. Some popular options include \"Python Crash Course\" by Eric Matthes and \"Automate the Boring Stuff with Python\" by Al Sweigart.\\n\\n3. Practice coding: The best way to learn Python is by practicing coding regularly. Try to solve coding challenges on websites like LeetCode or HackerRank to improve your problem-solving skills.\\n\\n4. Join a community: Joining online forums or local meetups can help you connect with other Python enthusiasts and get support as you learn. Websites like Stack Overflow and Reddit have active Python communities where you can ask questions and seek advice.\\n\\n5. Build projects: Once you have a good grasp of the basics, start working on small projects to apply your knowledge. Building projects will help you solidify your understanding of Python and gain practical experience.\\n\\nRemember, learning Python is a journey, so be patient with yourself and keep practicing regularly. Good luck!' response_metadata={'token_usage': {'completion_tokens': 262, 'prompt_tokens': 27, 'total_tokens': 289}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ad0953e8-e436-4b7b-acaf-534562f56cd0-0' usage_metadata={'input_tokens': 27, 'output_tokens': 262, 'total_tokens': 289}\n"
     ]
    }
   ],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I learn python?')\n",
    "message = llm(full_prompt)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de14ea9-26b7-4db6-b387-d046b866e9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Python can be a rewarding experience! Here are some steps you can take to start learning Python:\n",
      "\n",
      "1. Online tutorials and courses: There are many online resources available for learning Python, such as Codecademy, Coursera, and Udemy. These platforms offer interactive lessons and exercises to help you grasp the basics of Python.\n",
      "\n",
      "2. Books: There are numerous books on Python programming that cater to different skill levels. Some popular options include \"Python Crash Course\" by Eric Matthes and \"Automate the Boring Stuff with Python\" by Al Sweigart.\n",
      "\n",
      "3. Practice coding: The best way to learn Python is by practicing coding regularly. Try to solve coding challenges on websites like LeetCode or HackerRank to improve your problem-solving skills.\n",
      "\n",
      "4. Join a community: Joining online forums or local meetups can help you connect with other Python enthusiasts and get support as you learn. Websites like Stack Overflow and Reddit have active Python communities where you can ask questions and seek advice.\n",
      "\n",
      "5. Build projects: Once you have a good grasp of the basics, start working on small projects to apply your knowledge. Building projects will help you solidify your understanding of Python and gain practical experience.\n",
      "\n",
      "Remember, learning Python is a journey, so be patient with yourself and keep practicing regularly. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a023832-fd3a-4e9a-8d22-2de044cb9d62",
   "metadata": {},
   "source": [
    "**ChatMessageHistory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c11442-f5f9-45ff-b749-3ffc2c83d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.2.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (0.1.82)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (2.7.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (24.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (2.18.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.2/49.2 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a1545b-8df1-4ac9-bded-e263cb3fa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa1945b-f02b-4ab1-ba32-9678ec345f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The system boundary in LCA defines the scope of the analysis by delineating the inputs, outputs, and processes included in the assessment.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 33, 'total_tokens': 60}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5f45af59-4717-415a-9c27-f153a993459b-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hi! Ask me anything about LCA.\")  #set the tone and direction of the conversation.\n",
    "history.add_user_message(\"Describe the meaning of system boundary in LCA as one sentence.\")  #add user messages\n",
    "chat(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e1b4c8-18f9-400e-b63a-eee6021d8efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='System boundary defines the scope of the analysis.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 47, 'total_tokens': 56}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-38c65f52-2671-496d-9e26-5e1a4cff3227-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_user_message(\"Summarize the preceding sentence in fewer words.\")  #response based on the full context stored in the conversation history.\n",
    "chat(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2798-53ba-48a9-be05-b8cd2337b1ec",
   "metadata": {},
   "source": [
    "**ConversationBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ad1464-2a84-4268-b116-e976290b32b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe a language model in one sentence\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:  A language model is a program that generates sentences from text patterns.\n",
      "Human: Describe it again in one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:  A language model is a program that generates sentences from text patterns.\n",
      "Human: Describe it again in one word\n",
      "AI:  Predictive.\n",
      "Human: What did I first ask you? I forgot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You asked me to describe a language model in one sentence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "memory = ConversationBufferMemory(size=4)  #the number of messages to store with the size argument.\n",
    "\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)  #conversation Chain. Use verbose=True: display the output and memory state for each question.\n",
    "\n",
    "buffer_chain.predict(input=\"Describe a language model in one sentence\")\n",
    "buffer_chain.predict(input=\"Describe it again using less words\")\n",
    "buffer_chain.predict(input=\"Describe it again fewer words but at least one word\")\n",
    "buffer_chain.predict(input=\"Describe it again in one word\")\n",
    "buffer_chain.predict(input=\"What did I first ask you? I forgot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefad1-c001-471b-87f8-f9e6e207b4bd",
   "metadata": {},
   "source": [
    "**ConversationSummaryMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e598b3-e2db-4ebc-8f81-1f2beded0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: please summarize the future in 2 sentences.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI to summarize the future in two sentences. The AI explains that the future is constantly changing and impossible to summarize in just two sentences, but it holds endless possibilities for growth and progress.\n",
      "Human: why?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI to summarize the future in two sentences. The AI explains that the future is constantly changing and impossible to summarize in just two sentences, but it holds endless possibilities for growth and progress due to the ever-evolving nature of the world and the potential for humanity to advance in various aspects of life.\n",
      "Human: What will I need to shape this?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' To shape the future, you will need a combination of determination, creativity, and adaptability. It will also be important to stay informed and open-minded, as the future is constantly changing and new challenges and opportunities will arise.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key))\n",
    "\n",
    "Summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "Summary_chain.predict(input=\"please summarize the future in 2 sentences.\")\n",
    "Summary_chain.predict(input=\"why?\")\n",
    "Summary_chain.predict(input=\"What will I need to shape this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102afd3d-ba69-4260-94c3-e88ee4c3d297",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82049c8-3f33-4062-a22d-255c09fab069",
   "metadata": {},
   "source": [
    "**1. Load the document**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "747665b7-acad-426a-9c13-48ac7334743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda6c7d9-5a75-4a03-bfc0-4cc483cdc858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path attention_is_all_you_need.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PDF document loader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m----> 4\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_is_all_you_need.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()  \u001b[38;5;66;03m# Load the document\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Print the first document\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:182\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[1;32mD:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:116\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[1;34m(self, file_path, headers)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[1;31mValueError\u001b[0m: File path attention_is_all_you_need.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "#PDF document loader\n",
    "loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")  \n",
    "\n",
    "data = loader.load()  # Load the document\n",
    "print(data[0])  # Print the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166c846-79b5-4e41-8ec8-5c0d5e99249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV document loader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path = 'fifa_countries_audience.csv')\n",
    "\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4702a0-bc24-437b-98b2-0dec66a0d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#third-party document loader\n",
    "from langchain_community.document_loaders import HNLoader\n",
    "\n",
    "loader = HNLoader(web_path='https://news.ycombinator.com')\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2be742-906f-4a1f-b5e9-3d5743522082",
   "metadata": {},
   "source": [
    "**2. Splitting external data for retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1853652-30e1-4587-a8f1-f7dce542c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans.\\\n",
    "No machine can do the work of one extraordinary human.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e1b867-442a-4603-9f66-2002d312041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5bf6a-f697-4f5a-9d7e-aa73c011fa27",
   "metadata": {},
   "source": [
    "**CharacterTextSplitter to split docuemnts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee26617-03b9-4683-b676-fcf4ef57c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans', 'No machine can do the work of one extraordinary human']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size=24\n",
    "chunk_overlap=3\n",
    "\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "    separator='.',\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = ct_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d797b2d-48a5-482c-9c0f-170c603869a5",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4519d6ed-1695-40b7-b936-7a1c7f5064c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the', 'work of fifty ordinary', 'humans.No machine can', 'do the work of one', 'extraordinary human.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed8b01-2ffb-4d96-909b-83b1fe56a561",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter with HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f722745-03c8-44c5-9f40-f53b7755ca57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      4\u001b[0m load \u001b[38;5;241m=\u001b[39m UnstructuredHTMLLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhite_house_executive_order_nov_2023.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      7\u001b[0m rc_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m      8\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[0;32m      9\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39mchunk_overlap,\n\u001b[0;32m     10\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m docs \u001b[38;5;241m=\u001b[39m rc_splitter\u001b[38;5;241m.\u001b[39msplit_documents(data)  \u001b[38;5;66;03m#for non-string formats, use the split_documents.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load = UnstructuredHTMLLoader(\"White_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.']\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_documents(data)  #for non-string formats, use the split_documents.\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c0044-ca37-4a1f-be14-ba7ea4229b09",
   "metadata": {},
   "source": [
    "**3. RAG storage and retrieval using vector databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445c1eb5-46f1-48f5-b87f-09e495787081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a kindom of lychee']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = 'There is a kindom of lychee'\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 40\n",
    "chunk_overlap = 10\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = splitter.split_text(quote)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e432fa9-8744-49c6-ac89-f2761146041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a Chroma vector database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "docstorage = Chroma.from_texts(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd3a7dda-8236-46d1-ab4d-886efd7f4b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RetrieveQA' from 'langchain.chains' (D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain\\chains\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#RAG generation using a vector database\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrieveQA\n\u001b[0;32m      4\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[0;32m      5\u001b[0m                                            chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mdocstorage\u001b[38;5;241m.\u001b[39mas_retriever()))  \u001b[38;5;66;03m#chain_type: optimize for different document structures.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere do lychee fruite live?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RetrieveQA' from 'langchain.chains' (D:\\Software\\Anaconda\\envs\\LCI-extraction\\Lib\\site-packages\\langchain\\chains\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#RAG generation using a vector database\n",
    "from langchain.chains import RetrieveQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key,\n",
    "                                           chain_type=\"stuff\", retriever=docstorage.as_retriever()))  #chain_type: optimize for different document structures.\n",
    "\n",
    "query = \"Where do lychee fruite live?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd63dd-2906-4495-b7b3-f835005edf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RetrieveQAWithSourcesChain: returns the data source of the answer.\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa = RetrievaQAWithSourcesChain.from_Chain_type(\n",
    "    \n",
    ")\n",
    "\n",
    "results = qa\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9709ce9-822c-4fa7-97d9-9d7471feb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"What is the primary architecture presented in the document?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf40d8-9080-4736-a7a0-2bd0197767d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the function for the question to be answered with\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the query on the documents\n",
    "results = qa({\"question\": \"What is the gprimary architecture presented in the document?\"}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4e75e-b40b-4cf9-9262-be7635f2b834",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7247ce4d-e28f-4ae1-a5b2-5ecfd8ba8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31eaf72c-c8af-4cbe-aa87-b1f3b59b4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"1. Set clear goals: Define what you want to achieve in your studies and break it down into smaller, manageable tasks. This will give you a sense of direction and purpose.\\n\\n2. Create a study schedule: Make a study timetable that includes dedicated time for each subject or topic. Having a structured routine can help you stay on track and maintain motivation.\\n\\n3. Take regular breaks: It's important to give yourself time to rest and recharge your mind. Schedule short breaks in between study sessions to prevent burnout.\\n\\n4. Stay organized: Keep your study materials and notes organized so you can easily access them when needed. This will help you stay focused and avoid feeling overwhelmed.\\n\\n5. Find a study buddy: Studying with a friend or classmate can make the process more enjoyable and help keep you accountable. You can motivate each other and share ideas and study tips.\\n\\n6. Reward yourself: Set up a system of rewards for reaching study milestones or completing tasks. Treat yourself to something you enjoy, whether it's a movie night, a favorite snack, or some free time to relax.\\n\\n7. Stay positive: Remember to celebrate your achievements, no matter how small. Stay positive and remind yourself of your goals and the reasons why you are studying in the first place.\\n\\n8. Seek support: If you are struggling with motivation, don't be afraid to ask for help. Talk to your teachers, classmates, or a counselor for advice and support. Having a support system can help you stay motivated and on track with your studies.\", response_metadata={'token_usage': {'completion_tokens': 306, 'prompt_tokens': 27, 'total_tokens': 333}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f8a31774-a49d-42c9-b0e9-4a8add144cf8-0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a helpful personal assistant.\\\n",
    "    Answer the following question: {question}\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "message = chain.invoke({\"question\":\"how to keep motivation in study?\"})\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d59cfa3-351b-4974-b273-284bfae69308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In\n",
      " the\n",
      " context\n",
      " of\n",
      " lang\n",
      "chain\n",
      ",\n",
      " \"\n",
      "chain\n",
      ".stream\n",
      "\"\n",
      " likely\n",
      " refers\n",
      " to\n",
      " a\n",
      " method\n",
      " or\n",
      " function\n",
      " within\n",
      " the\n",
      " programming\n",
      " language\n",
      " that\n",
      " is\n",
      " used\n",
      " to\n",
      " process\n",
      " or\n",
      " manipulate\n",
      " data\n",
      " streams\n",
      " within\n",
      " a\n",
      " blockchain\n",
      " network\n",
      ".\n",
      " This\n",
      " could\n",
      " involve\n",
      " tasks\n",
      " such\n",
      " as\n",
      " reading\n",
      " from\n",
      " or\n",
      " writing\n",
      " to\n",
      " a\n",
      " stream\n",
      ",\n",
      " filtering\n",
      " data\n",
      ",\n",
      " or\n",
      " performing\n",
      " other\n",
      " operations\n",
      " on\n",
      " the\n",
      " data\n",
      " flowing\n",
      " through\n",
      " the\n",
      " blockchain\n",
      " network\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "#.stream() method can be used to stream the response to the end user in iterative chunks.\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"What's the meaning of chain.stream in langchain?\"}):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bff8d11-da39-48bd-b4ff-008907bde6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In computer science, a batch refers to a collection of jobs or tasks that are processed together as a group. This can involve running multiple programs or commands sequentially without manual intervention. Batch processing is commonly used for tasks such as data processing, printing documents, or updating software.\n"
     ]
    }
   ],
   "source": [
    "#Batching\n",
    "inputs = [{\"question\": \"What is langchain\",\n",
    "           \"question\": \"What meaning of batch in CS\"}]\n",
    "\n",
    "results = chain.batch(inputs)\n",
    "for result in results:\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d839cf07-879a-4992-b5db-f832e3d4f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runnables in LCEL: functions or actions executed during the expression\n",
    "#RunnablePassThrough: pass inputs to the model.\n",
    "#RunnableLambda: transform inputs.\n",
    "#RunnableMap: processing inputs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd0a68fd-5fda-4fe9-92f1-a66f0e7dc228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inputs in LCI include energy, water, resources, and land.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "vectorstore = Chroma.from_texts([\"\"\"When an LCA is performed, a practitioner will set metrics to quantify the different inputs (e.g., energy, water, resources, land) and outputs (e.g., emissions, wastes, products) that occur throughout the life cycle of an industrial process, technology, or commodity. \n",
    "                                It allows an assessor to map flows of energy, resources, and materials in and out of a system. These are objective measurements, tracking distinct quantities like volume, mass, or weight. \n",
    "                                They are collected as part of the life cycle inventory (LCI).\"\"\"],\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Anwer the question based on the context:{context}. Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = ({\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | model | StrOutputParser())\n",
    "chain.invoke(\"What is input in LCI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f55b4-817b-4623-8e86-7fb5c480d615",
   "metadata": {},
   "source": [
    "**Implementing functional LangChain chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cec59d8a-6b98-45d6-87c6-16302c634be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在中国，最受欢迎的饮料之一是白酒，这是一种通常用高粱或其他谷物制成的浓烈蒸馏酒。它通常干净地享用或与其他饮料混合，是许多社交聚会和庆祝活动的必备品。如果您正在寻找独特和地道的中国饮酒体验，白酒是必须尝试的！.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequential Chains\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "bartender_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert bartender. Mention the most popular drink in your region. question: {question}. Your answer:\"\"\")\n",
    "translation_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert translator. Translate an order of the drink in the language native to drink. Item: {answer}. Your translation:\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "chain = ({\"answer\": bartender_prompt | llm | StrOutputParser()}\n",
    "         | translation_prompt\n",
    "         | llm\n",
    "         | StrOutputParser())\n",
    "\n",
    "chain.invoke({\"question\": \"I an in China, what's good to drink?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "370ecad8-0554-455d-9a37-6d20f611c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated number: 72\n",
      "Result of multiplication: 72 * 2 = 144\n"
     ]
    }
   ],
   "source": [
    "#Manipulating values with sequential chains\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Generate a random number\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Multiply {number} by 2\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain1 = prompt1 | llm\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "response1 = chain1.invoke({})\n",
    "response2 = chain2.invoke({\"number\": response1.content})\n",
    "\n",
    "print(\"Generated number:\", response1.content)\n",
    "print(\"Result of multiplication:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65110abf-6c23-4fe5-9536-c64eb5a1dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response:\n",
      "The best ice cream flavor is subjective and based on personal preference. Some popular flavors include vanilla, chocolate, mint chocolate chip, and cookie dough. Ultimately, the best ice cream is the one that you enjoy the most.\n",
      "\n",
      "Opposing response:\n",
      "The statement that technology has improved our lives in countless ways is challenged by the argument that technology has actually had a negative impact on society. This opposing perspective highlights how technology has increased social isolation, perpetuated inequality, and created a reliance on screens that is detrimental to physical and mental health.\n"
     ]
    }
   ],
   "source": [
    "#RunnablePassthrough in chains: passing values between chains\n",
    "#Example1\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "q_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a helpful assistant. Answer the question: {input}\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "contrarian_response = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"You are a contrarian. Describe the most powerful opposing perspective for {response}\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain = (\n",
    "    {\"response\": q_response, \"opposing_response\": contrarian_response}\n",
    "    | ChatPromptTemplate.from_messages(\n",
    "        [(\"ai\", \"{response}\"),\n",
    "         (\"human\", \"Response: \\n{response}\\n\\nOpposing response:\\n{opposing_response}\"),\n",
    "         (\"system\", \"Summarize the original response and an opposing response.\")])\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_chain.invoke({\"input\": \"What is the best ice cream?\", \"response\":\"\", \"opposing_response\":\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a2692c5-164c-4d8b-b89a-fe4e2d75ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing cutting-edge artificial intelligence technology, our voice-to-text software allows users to easily dictate their messages, emails, and notes on their mobile devices with unparalleled accuracy and efficiency. This technology not only saves users time and effort but also reduces the risk of errors commonly associated with traditional typing methods.\n",
      "\n",
      "Through extensive market research, we have identified a growing demand for hands-free typing solutions in the mobile space, particularly among busy professionals and individuals with limited dexterity. By developing a product that addresses this need, we are able to tap into a lucrative market opportunity with significant growth potential.\n",
      "\n",
      "Our product development team has worked tirelessly to fine-tune our software, ensuring that it delivers a seamless and intuitive user experience. By incorporating advanced machine learning algorithms, our software continuously learns and adapts to user preferences, further enhancing its accuracy and efficiency over time.\n",
      "\n",
      "In addition to our innovative technology, we have also invested heavily in financial planning to ensure the long-term sustainability and profitability of our business. By carefully analyzing market trends, forecasting revenue projections, and managing expenses effectively, we have positioned ourselves for success in a competitive and rapidly evolving industry.\n",
      "\n",
      "Overall, our AI-powered voice-to-text software represents a game-changing innovation in the mobile typing space. With a solid foundation built on market research, product development, and financial planning, we are confident in our ability to drive profitability and sustainability for years to come.\n"
     ]
    }
   ],
   "source": [
    "# Example2: Make ceo_response available for other chains\n",
    "ceo_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a CEO. Describe the most lucrative consumer product addressing the following consumer need in one sentence: {input}.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"ceo_response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "advisor_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a strategic adviser. Briefly map the outline and business plan for {ceo_response} in 3 key steps.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "overall_response = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"CEO response:\\n{ceo_response}\\n\\nAdvisor response:\\n{advisor_response}\"),\n",
    "            (\"system\", \"Generate a final response including the CEO's response, the advisor response, and a summary of the business plan in one sentence.\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain to insert the outputs from the other chains into overall_response\n",
    "business_idea_chain = (\n",
    "    {\"ceo_response\": ceo_response, \"advisor_response\": advisor_response}\n",
    "    | overall_response\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(business_idea_chain.invoke({\"input\": \"Typing on mobile touchscreens is slow.\", \"ceo_response\": \"\", \"advisor_response\": \"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ba01-41d8-4f97-bafc-d2f8c41c4935",
   "metadata": {},
   "source": [
    "**An introduction to LangChain agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fa4f4db6-8cf1-4c9d-a9c8-a6b34c53f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.1-cp312-cp312-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.1-cp312-cp312-win_amd64.whl (141 kB)\n",
      "   ---------------------------------------- 0.0/141.3 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 20.5/141.3 kB 640.0 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 92.2/141.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 141.3/141.3 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6546c14f-058d-4e6f-87e8-26400e78b1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this problem.\n",
      "Action: Calculator\n",
      "Action Input: 10 * 50\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 500\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 500\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'500'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Zero-shot ReAct agent (reasoning and acting)\n",
    "# Zero-Shot ReAct agent in LangChain is an agent type that can generate responses to user inputs without explicit training on specific tasks.\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "# Define the tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Define the agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What is 10 mutiplied by 50?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f01a6-1f8b-4c9d-9e25-f0a34da8fd79",
   "metadata": {},
   "source": [
    "**Utilizing tools in LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e70ec98c-01d9-43e1-801a-1dbedf05a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A custom tool example: financial reporting\n",
    "\n",
    "@tool\n",
    "def financial_report(company_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a basic financial report for a company.\n",
    "\n",
    "    Args:\n",
    "    - company_name (str): The name of the company.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted financial report.\n",
    "    \"\"\"\n",
    "    revenue = 1000000\n",
    "    expenses = 500000\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Extremely Basic Financial Report including net income for {company_name}\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "\n",
    "    return report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420b346e-39cd-475f-9e2a-191ec78d9bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the FinanceReport tool to calculate net income\n",
      "Action: FinanceReport\n",
      "Action Input: 'Apple'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mExtremely Basic Financial Report including net income for 'Apple'\n",
      "Revenue: $1000000\n",
      "Expenses: $500000\n",
      "Net Income: $500000\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should subtract expenses from revenue to get net income\n",
      "Action: Subtract\n",
      "Action Input: $500000 - $1000000\u001b[0m\n",
      "Observation: Subtract is not a valid tool, try one of [FinanceReport].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the FinanceReport tool to calculate net income\n",
      "Action: FinanceReport\n",
      "Action Input: 'Apple'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mExtremely Basic Financial Report including net income for 'Apple'\n",
      "Revenue: $1000000\n",
      "Expenses: $500000\n",
      "Net Income: $500000\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The net income for Apple is $500000.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The net income for Apple is $500000.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the custom tool\n",
    "#Import libraries\n",
    "from langchain.agents import tool, AgentType, Tool, initialize_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "#Define the previously created tool in a list\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"FinanceReport\",\n",
    "        func=financial_report,\n",
    "        description=\"Use this for running a financial report for net income.\",)]\n",
    "\n",
    "#Define the model and the agent\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "#Define a question and run the agent\n",
    "question = \"Run a financial report that calculates net income for Apple\"\n",
    "agent.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d1aafb-0780-4712-8bbc-eb7c6965a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A structuredTool example: division\n",
    "#It allows specifically define the formats of input and output variables so they are consistent.\n",
    "def divisible_by_five(n: int) -> int:\n",
    "    \"\"\"Calculate the number of times an input is divisible by five.\"\"\"\n",
    "    n_times = n // 5\n",
    "    return n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7d0140-69d4-4831-b349-556eb6b395fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "factorial_tool = StructuredTool.from_function(divisible_by_five)\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(tools=[factorial_tool], llm=llm, \n",
    "        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "result = factorial_tool.func(n=100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28989b8a-78f1-4d7a-a83c-731cff6e12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n"
     ]
    }
   ],
   "source": [
    "#Example2\n",
    "def calculate_wellness_score(sleep_hours, exercise_minutes, healthy_meals, stress_level):\n",
    "    \"\"\"Calculate a Wellness Score based on sleep, exercise, nutrition, and stress management.\"\"\"\n",
    "    max_score_per_category = 25\n",
    "\n",
    "    sleep_score = min(sleep_hours / 8 * max_score_per_category, max_score_per_category)\n",
    "    exercise_score = min(exercise_minutes / 30 * max_score_per_category, max_score_per_category)\n",
    "    nutrition_score = min(healthy_meals / 3 * max_score_per_category, max_score_per_category)\n",
    "    stress_score = max_score_per_category - min(stress_level / 10 * max_score_per_category, max_score_per_category)\n",
    "\n",
    "    total_score = sleep_score + exercise_score + nutrition_score + stress_score\n",
    "    return total_score\n",
    "\n",
    "# Create a structured tool from calculate_wellness_score()\n",
    "tools = [StructuredTool.from_function(calculate_wellness_score)]\n",
    "\n",
    "# Initialize the appropriate agent type and tool set\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "wellness_tool = tools[0]\n",
    "result = wellness_tool.func(sleep_hours=8, exercise_minutes=30, healthy_meals=10, stress_level=20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0336e4-373a-4c9b-99b9-2275d075d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company_name': {'title': 'Company Name', 'type': 'string'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_report.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9fc96ed-6a7c-49da-9afa-72e3531f9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting tools for OpenAI models\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class FinancialReportDescription(BaseModel):\n",
    "    query: str = Field(description=\"generate a financial report using net income\")\n",
    "\n",
    "@tool(args_schema=FinancialReportDescription)\n",
    "def financial_report(company_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a basic financial report for a company.\n",
    "\n",
    "    Args:\n",
    "    - company_name (str): The name of the company.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted financial report.\n",
    "    \"\"\"\n",
    "    revenue = 1000000\n",
    "    expenses = 500000\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Extremely Basic Financial Report including net income for {company_name}\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "\n",
    "    return report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53531fcf-c9de-441b-9a3c-f656c7919181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'financial_report', 'description': 'Generate a basic financial report for a company.\\n\\nArgs:\\n- company_name (str): The name of the company.\\n\\nReturns:\\n- str: The formatted financial report.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'generate a financial report using net income', 'type': 'string'}}, 'required': ['query']}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import format_tool_to_openai_function\n",
    "\n",
    "print(format_tool_to_openai_function(financial_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "576870a3-ea59-443c-bdd0-2d4566acaa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'calculate_ltv', 'description': 'Generate the LTV for a company to pontificate with.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'Calculate an extremely simple historical LTV', 'type': 'string'}}, 'required': ['query']}}\n"
     ]
    }
   ],
   "source": [
    "#Example2\n",
    "# Create an LTVDescription class to manually add a function description\n",
    "class LTVDescription(BaseModel):\n",
    "    query: str = Field(description='Calculate an extremely simple historical LTV')\n",
    "\n",
    "# Format the calculate_ltv tool function so it can be used by OpenAI models\n",
    "@tool(args_schema=LTVDescription)\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company to pontificate with.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"Pontification Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "print(format_tool_to_openai_function(calculate_ltv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2c080-55a1-4b50-a5ad-528fd05400c1",
   "metadata": {},
   "source": [
    "**Troubleshooting methods for optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05622e02-129b-4c0b-a735-05bd7f2afedc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What does space smell like?']\n",
      "gpt-3.5-turbo-instruct\n",
      "0.0\n",
      "\n",
      "\n",
      "Space does not have a distinct smell as it is a vacuum and does not contain any particles or molecules that can produce a scent. Astronauts have reported a metallic or burnt smell when returning from spacewalks, which is likely due to the lingering odor of their spacesuits and equipment.\n"
     ]
    }
   ],
   "source": [
    "#A callback example\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class CallingItBack(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, invocation_params, **kwargs):\n",
    "        print(prompts)\n",
    "        print(invocation_params[\"model_name\"])\n",
    "        print(invocation_params[\"temperature\"])\n",
    "\n",
    "    def on_llm_new_token(self, token:str, **kwargs) -> None:\n",
    "        print(repr(token))\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "prompt_template = \"What does {thing} smell like?\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "output = chain.run({\"thing\": \"space\"}, callbacks=[CallingItBack()])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a3bdfb4-8dca-4bf6-9fb1-06e7a8871a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The identity of the Walrus is a subject of much debate and speculation. In order to determine who the Walrus is, we must first analyze the context in which the question is being asked. \n",
      "\n",
      "If we are referring to the character in the Beatles song \"I Am the Walrus,\" it is widely believed that the Walrus represents John Lennon himself. This interpretation is supported by Lennon's own statements about the song and its lyrics. \n",
      "\n",
      "However, if we are considering the Walrus in a broader sense, we must delve into the symbolism and cultural significance of the animal itself. The Walrus is often associated with wisdom, strength, and adaptability in various mythologies and folklore. \n",
      "\n",
      "Therefore, in order to definitively answer the question of who the Walrus is, we must carefully consider the specific context in which the term is being used and conduct a thorough analysis of the symbolism and meaning associated with the Walrus in that context.\n"
     ]
    }
   ],
   "source": [
    "#Using the verbose flag to debug complex decisions\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(streaming=True, openai_api_key=openai_api_key, temperature=0, verbose=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer a question with a strict process and deep analysis: {question}\")\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"question\": \"Who is the Walrus?\"})\n",
    "output = response.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d93e695-e7bb-4e66-ab2d-940153382982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '\\n\\n' generated at time: 1719641061.8714874\n",
      "Token: 'Photos' generated at time: 1719641061.8744864\n",
      "Token: 'ynthesis' generated at time: 1719641061.8754873\n",
      "Token: ' is' generated at time: 1719641061.9659865\n",
      "Token: ' the' generated at time: 1719641061.9659865\n",
      "Token: ' process' generated at time: 1719641061.9669852\n",
      "Token: ' by' generated at time: 1719641062.0469959\n",
      "Token: ' which' generated at time: 1719641062.0469959\n",
      "Token: ' plants, algae,' generated at time: 1719641062.0469959\n",
      "Token: ' and' generated at time: 1719641062.1007159\n",
      "Token: ' some' generated at time: 1719641062.1007159\n",
      "Token: ' bacteria' generated at time: 1719641062.1007159\n",
      "Token: ' convert light' generated at time: 1719641062.1007159\n",
      "Token: ' energy' generated at time: 1719641062.3398612\n",
      "Token: ' from' generated at time: 1719641062.3398612\n",
      "Token: ' the' generated at time: 1719641062.4092112\n",
      "Token: ' sun' generated at time: 1719641062.4102108\n",
      "Token: ' into chemical energy in the form of glucose.' generated at time: 1719641062.4102108\n",
      "Token: ' This process takes' generated at time: 1719641062.4199233\n",
      "Token: ' place' generated at time: 1719641062.4199233\n",
      "Token: ' in' generated at time: 1719641062.420927\n",
      "Token: ' the' generated at time: 1719641062.4880018\n",
      "Token: ' chlor' generated at time: 1719641062.4890075\n",
      "Token: 'oplast' generated at time: 1719641062.4900177\n",
      "Token: 's' generated at time: 1719641062.5449927\n",
      "Token: ' of plant cells' generated at time: 1719641062.5459912\n",
      "Token: '.\\n\\n' generated at time: 1719641062.5459912\n",
      "Token: '1' generated at time: 1719641062.6149461\n",
      "Token: '.' generated at time: 1719641062.6149461\n",
      "Token: ' Abs' generated at time: 1719641062.6159499\n",
      "Token: 'orption' generated at time: 1719641062.661673\n",
      "Token: ' of' generated at time: 1719641062.6626801\n",
      "Token: ' Light' generated at time: 1719641062.6626801\n",
      "Token: ' Energy' generated at time: 1719641062.7160885\n",
      "Token: ':' generated at time: 1719641062.7160885\n",
      "Token: ' The' generated at time: 1719641062.7170856\n",
      "Token: ' first' generated at time: 1719641062.8525524\n",
      "Token: ' step' generated at time: 1719641062.8535538\n",
      "Token: ' in' generated at time: 1719641062.8535538\n",
      "Token: ' photos' generated at time: 1719641062.8886409\n",
      "Token: 'ynthesis' generated at time: 1719641062.8896394\n",
      "Token: ' is the absorption of' generated at time: 1719641062.8896394\n",
      "Token: ' light' generated at time: 1719641062.954156\n",
      "Token: ' energy' generated at time: 1719641062.954156\n",
      "Token: ' by' generated at time: 1719641062.955151\n",
      "Token: ' pig' generated at time: 1719641062.955151\n",
      "Token: 'ments' generated at time: 1719641062.9755123\n",
      "Token: ',' generated at time: 1719641062.9755123\n",
      "Token: ' such' generated at time: 1719641063.0220044\n",
      "Token: ' as' generated at time: 1719641063.0230086\n",
      "Token: ' chlor' generated at time: 1719641063.0230086\n",
      "Token: 'oph' generated at time: 1719641063.0230086\n",
      "Token: 'yll' generated at time: 1719641063.0910342\n",
      "Token: ',' generated at time: 1719641063.0910342\n",
      "Token: ' found' generated at time: 1719641063.101446\n",
      "Token: ' in' generated at time: 1719641063.101446\n",
      "Token: ' the' generated at time: 1719641063.1024392\n",
      "Token: ' chlor' generated at time: 1719641063.1024392\n",
      "Token: 'oplast' generated at time: 1719641063.193277\n",
      "Token: 's' generated at time: 1719641063.193277\n",
      "Token: '.' generated at time: 1719641063.1942601\n",
      "Token: ' These' generated at time: 1719641063.1952622\n",
      "Token: ' pig' generated at time: 1719641063.1952622\n",
      "Token: 'ments' generated at time: 1719641063.1962583\n",
      "Token: ' are' generated at time: 1719641063.2608197\n",
      "Token: ' responsible' generated at time: 1719641063.2608197\n",
      "Token: ' for' generated at time: 1719641063.279889\n",
      "Token: ' capturing' generated at time: 1719641063.279889\n",
      "Token: ' the' generated at time: 1719641063.279889\n",
      "Token: ' energy' generated at time: 1719641063.431308\n",
      "Token: ' from' generated at time: 1719641063.431308\n",
      "Token: ' sunlight' generated at time: 1719641063.431308\n",
      "Token: '.\\n\\n' generated at time: 1719641063.4461908\n",
      "Token: '2.' generated at time: 1719641063.4471917\n",
      "Token: ' Conversion of' generated at time: 1719641063.4477134\n",
      "Token: ' Light' generated at time: 1719641063.5872622\n",
      "Token: ' Energy' generated at time: 1719641063.5882764\n",
      "Token: ' to' generated at time: 1719641063.5882764\n",
      "Token: ' Chemical' generated at time: 1719641063.588786\n",
      "Token: ' Energy: The absorbed light' generated at time: 1719641063.669473\n",
      "Token: ' energy' generated at time: 1719641063.669473\n",
      "Token: ' is' generated at time: 1719641063.669473\n",
      "Token: ' then' generated at time: 1719641063.7502584\n",
      "Token: ' converted' generated at time: 1719641063.7512577\n",
      "Token: ' into chemical energy' generated at time: 1719641063.8073108\n",
      "Token: ' in' generated at time: 1719641063.8083227\n",
      "Token: ' the' generated at time: 1719641063.8083227\n",
      "Token: ' form' generated at time: 1719641063.8083227\n",
      "Token: ' of' generated at time: 1719641063.8605611\n",
      "Token: ' ATP' generated at time: 1719641063.8615701\n",
      "Token: ' (' generated at time: 1719641063.9773302\n",
      "Token: 'aden' generated at time: 1719641063.9773302\n",
      "Token: 'os' generated at time: 1719641063.978348\n",
      "Token: 'ine' generated at time: 1719641064.114887\n",
      "Token: ' tri' generated at time: 1719641064.114887\n",
      "Token: 'ph' generated at time: 1719641064.1158848\n",
      "Token: 'osphate' generated at time: 1719641064.1158848\n",
      "Token: ')' generated at time: 1719641064.1840134\n",
      "Token: ' and NADPH (' generated at time: 1719641064.1850264\n",
      "Token: 'nic' generated at time: 1719641064.1860292\n",
      "Token: 'ot' generated at time: 1719641064.2020931\n",
      "Token: 'in' generated at time: 1719641064.2020931\n",
      "Token: 'amide' generated at time: 1719641064.2810235\n",
      "Token: ' aden' generated at time: 1719641064.2820246\n",
      "Token: 'ine' generated at time: 1719641064.2830265\n",
      "Token: ' din' generated at time: 1719641064.2830265\n",
      "Token: 'ucle' generated at time: 1719641064.3532379\n",
      "Token: 'otide' generated at time: 1719641064.3542402\n",
      "Token: ' phosphate). These molecules' generated at time: 1719641064.4220934\n",
      "Token: ' are' generated at time: 1719641064.423093\n",
      "Token: ' used' generated at time: 1719641064.423093\n",
      "Token: ' to power the' generated at time: 1719641064.490528\n",
      "Token: ' next' generated at time: 1719641064.490528\n",
      "Token: ' stage' generated at time: 1719641064.4915287\n",
      "Token: ' of' generated at time: 1719641064.5353198\n",
      "Token: ' photos' generated at time: 1719641064.5353198\n",
      "Token: 'ynthesis' generated at time: 1719641064.5363219\n",
      "Token: '.\\n\\n' generated at time: 1719641064.5922298\n",
      "Token: '3' generated at time: 1719641064.5922298\n",
      "Token: '.' generated at time: 1719641064.5932333\n",
      "Token: ' Split' generated at time: 1719641064.5942302\n",
      "Token: 'ting' generated at time: 1719641064.6621795\n",
      "Token: ' of' generated at time: 1719641064.6631887\n",
      "Token: ' Water' generated at time: 1719641064.7910244\n",
      "Token: ':' generated at time: 1719641064.792022\n",
      "Token: ' Water' generated at time: 1719641064.792022\n",
      "Token: ' molecules' generated at time: 1719641064.8120675\n",
      "Token: ' are then' generated at time: 1719641064.8120675\n",
      "Token: ' split into hydrogen ions' generated at time: 1719641064.8120675\n",
      "Token: ' (' generated at time: 1719641064.8750427\n",
      "Token: 'H' generated at time: 1719641064.8760438\n",
      "Token: '+)' generated at time: 1719641064.8760438\n",
      "Token: ' and' generated at time: 1719641064.8760438\n",
      "Token: ' oxygen' generated at time: 1719641064.8924992\n",
      "Token: ' (' generated at time: 1719641064.8934898\n",
      "Token: 'O' generated at time: 1719641064.9457052\n",
      "Token: '2' generated at time: 1719641064.9457052\n",
      "Token: ')' generated at time: 1719641064.9457052\n",
      "Token: ' through' generated at time: 1719641064.9909794\n",
      "Token: ' a' generated at time: 1719641064.9909794\n",
      "Token: ' process' generated at time: 1719641064.9909794\n",
      "Token: ' called' generated at time: 1719641065.0583217\n",
      "Token: ' phot' generated at time: 1719641065.0583217\n",
      "Token: 'ol' generated at time: 1719641065.0593216\n",
      "Token: 'ysis' generated at time: 1719641065.098635\n",
      "Token: '.' generated at time: 1719641065.098635\n",
      "Token: ' This' generated at time: 1719641065.098635\n",
      "Token: ' reaction' generated at time: 1719641065.135425\n",
      "Token: ' is' generated at time: 1719641065.135425\n",
      "Token: ' facilitated' generated at time: 1719641065.1369321\n",
      "Token: ' by' generated at time: 1719641065.1369321\n",
      "Token: ' the' generated at time: 1719641065.1772807\n",
      "Token: ' energy' generated at time: 1719641065.1782444\n",
      "Token: ' from' generated at time: 1719641065.2217946\n",
      "Token: ' ATP' generated at time: 1719641065.2217946\n",
      "Token: ' and' generated at time: 1719641065.2228017\n",
      "Token: ' N' generated at time: 1719641065.2228017\n",
      "Token: 'AD' generated at time: 1719641065.270208\n",
      "Token: 'PH' generated at time: 1719641065.271227\n",
      "Token: '.\\n\\n' generated at time: 1719641065.3144968\n",
      "Token: '4' generated at time: 1719641065.3144968\n",
      "Token: '.' generated at time: 1719641065.3144968\n",
      "Token: ' Formation' generated at time: 1719641065.3679876\n",
      "Token: ' of' generated at time: 1719641065.3679876\n",
      "Token: ' ATP' generated at time: 1719641065.3689942\n",
      "Token: ' and' generated at time: 1719641065.4052804\n",
      "Token: ' N' generated at time: 1719641065.4052804\n",
      "Token: 'AD' generated at time: 1719641065.4067903\n",
      "Token: 'PH' generated at time: 1719641065.4641738\n",
      "Token: ':' generated at time: 1719641065.4651723\n",
      "Token: ' The' generated at time: 1719641065.4651723\n",
      "Token: ' hydrogen' generated at time: 1719641065.4661741\n",
      "Token: ' ions' generated at time: 1719641065.5902684\n",
      "Token: ' and' generated at time: 1719641065.5912695\n",
      "Token: ' electrons' generated at time: 1719641065.6643999\n",
      "Token: ' from the split water molecules' generated at time: 1719641065.665401\n",
      "Token: ' are' generated at time: 1719641065.665401\n",
      "Token: ' used to' generated at time: 1719641065.718279\n",
      "Token: ' form ATP and' generated at time: 1719641065.7192805\n",
      "Token: ' N' generated at time: 1719641065.7192805\n",
      "Token: 'AD' generated at time: 1719641065.7598174\n",
      "Token: 'PH' generated at time: 1719641065.7608123\n",
      "Token: ',' generated at time: 1719641065.7608123\n",
      "Token: ' which' generated at time: 1719641065.8060184\n",
      "Token: ' are' generated at time: 1719641065.8070188\n",
      "Token: ' essential' generated at time: 1719641065.8070188\n",
      "Token: ' for' generated at time: 1719641065.842399\n",
      "Token: ' the' generated at time: 1719641065.842399\n",
      "Token: ' next' generated at time: 1719641065.8433342\n",
      "Token: ' stage' generated at time: 1719641065.9180324\n",
      "Token: ' of' generated at time: 1719641065.9180324\n",
      "Token: ' photos' generated at time: 1719641065.9180324\n",
      "Token: 'ynthesis' generated at time: 1719641065.9668841\n",
      "Token: '.\\n\\n' generated at time: 1719641065.9668841\n",
      "Token: '5' generated at time: 1719641065.9678943\n",
      "Token: '.' generated at time: 1719641066.1035452\n",
      "Token: ' Carbon' generated at time: 1719641066.1045449\n",
      "Token: ' D' generated at time: 1719641066.1045449\n",
      "Token: 'ioxide' generated at time: 1719641066.1299863\n",
      "Token: ' Fixation:' generated at time: 1719641066.1299863\n",
      "Token: ' In this stage,' generated at time: 1719641066.1299863\n",
      "Token: ' carbon dioxide' generated at time: 1719641066.1299863\n",
      "Token: ' (' generated at time: 1719641066.1299863\n",
      "Final Output: \n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose. This process takes place in the chloroplasts of plant cells.\n",
      "\n",
      "1. Absorption of Light Energy: The first step in photosynthesis is the absorption of light energy by pigments, such as chlorophyll, found in the chloroplasts. These pigments are responsible for capturing the energy from sunlight.\n",
      "\n",
      "2. Conversion of Light Energy to Chemical Energy: The absorbed light energy is then converted into chemical energy in the form of ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate). These molecules are used to power the next stage of photosynthesis.\n",
      "\n",
      "3. Splitting of Water: Water molecules are then split into hydrogen ions (H+) and oxygen (O2) through a process called photolysis. This reaction is facilitated by the energy from ATP and NADPH.\n",
      "\n",
      "4. Formation of ATP and NADPH: The hydrogen ions and electrons from the split water molecules are used to form ATP and NADPH, which are essential for the next stage of photosynthesis.\n",
      "\n",
      "5. Carbon Dioxide Fixation: In this stage, carbon dioxide (\n"
     ]
    }
   ],
   "source": [
    "# Complete the PerformanceMonitoringCallback class to return the token and time\n",
    "import time\n",
    "\n",
    "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    print(f\"Token: {repr(token)} generated at time: {time.time()}\")\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key, temperature=0, streaming=True)\n",
    "prompt_template = \"Describe the process of photosynthesis.\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Call the chain with the callback\n",
    "output = chain.run({}, callbacks=[PerformanceMonitoringCallback()])\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2e151-b208-4eeb-b054-f609f9e369b7",
   "metadata": {},
   "source": [
    "**Evaluating model output in LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5418b09-ced6-4809-b693-7c1f2db919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1. Is the submission relevant to the given input?\\n- The submission does not answer the question \"What is 26 + 43\" but instead provides information about the capital of New York state.\\n- The submission is not relevant to the given input.\\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "#using built-in evaluation criteria\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\",\n",
    "                         llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(prediction=\"The capital of New York state is Albany\",\n",
    "                                        input=\"What is 26 + 43\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fd0b71b-b691-4ccc-96d5-9fa8527fc894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"- market_potential: The submission does not effectively assess the market potential of the startup. It simply states that investing in a startup focused on flying cars is ridiculous without providing any analysis or justification.\\n- innovation: The submission does not highlight the startup's innovation and uniqueness in its sector. It dismisses the idea without acknowledging any potential innovative aspects of the startup.\\n- risk_assessment: The submission does not provide a thorough analysis of potential risks and mitigation strategies. It simply states that the idea is ridiculous without delving into specific risks or how they could be managed.\\n- scalability: The submission does not address the startup's scalability and growth potential. It only provides a blanket statement without considering the potential for growth or expansion.\", 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "#Custom evaluation criteria\n",
    "# Add a scalability criterion to custom_criteria\n",
    "custom_criteria = {\n",
    "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
    "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
    "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
    "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
    "}\n",
    "\n",
    "# Criteria an evaluator from custom_criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
    "    prediction=\"No, that is ridiculous.\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f554d649-9548-40f6-9484-cea24034f5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#QAEvalChain\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m(openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[0;32m      3\u001b[0m docstorage \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(docs, embedding)\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OpenAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#QAEvalChain\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding)\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# Generate the model responses using the RetrievalQA chain and question_set\n",
    "predictions = qa.apply(question_set)\n",
    "\n",
    "# Define the evaluation chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Evaluate the ground truth against the answers that are returned\n",
    "results = eval_chain.evaluate(question_set,\n",
    "                              predictions,\n",
    "                              question_key=\"question\",\n",
    "                              prediction_key=\"result\",\n",
    "                              answer_key='answer')\n",
    "\n",
    "for i, q in enumerate(question_set):\n",
    "    print(f\"Question {i+1}: {q['question']}\")\n",
    "    print(f\"Expected Answer: {q['answer']}\")\n",
    "    print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b95523-0139-4963-9cd6-fa4be403305b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
